{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9483d3ce",
   "metadata": {},
   "source": [
    "### Assignment 2 Understanding transfer learning and fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03187ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets\n",
    "import os \n",
    "import numpy as np \n",
    "from typing import Literal\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "print(f\"[INFO] Torch infos: {torch.__version__}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5535c",
   "metadata": {},
   "source": [
    "# We need transform from resnet18 and its weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ResNet18_Weights.DEFAULT.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e681a3e",
   "metadata": {},
   "source": [
    "# pick a dataset  you can import from\n",
    "\n",
    "```python\n",
    "from torchvision import datasets\n",
    "```\n",
    "\n",
    "# Easy picks:\n",
    "\n",
    "- Food101\n",
    "- Flowers102\n",
    "- DTD\n",
    "- FGVAircraft\n",
    "\n",
    "# Other picks:\n",
    "\n",
    "```python\n",
    "\n",
    "full_train = datasets.OxfordIIITPet(root=\"data\", split=\"trainval\", download=True, transform=transform)\n",
    "test_ds    = datasets.OxfordIIITPet(root=\"data\", split=\"test\", download=True, transform=transform)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9644c7a",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de646ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OxfordIIITPet: 37 pet breeds, good balance of classes\n",
    "# trainval split = 3680 images, test split = 3669 images\n",
    "\n",
    "full_train = datasets.OxfordIIITPet(root=\"data\", split=\"trainval\", download=True, transform=transform)\n",
    "test_ds    = datasets.OxfordIIITPet(root=\"data\", split=\"test\",     download=True, transform=transform)\n",
    "\n",
    "# Split full_train into train (80%) and validation (20%)\n",
    "train_size = int(0.8 * len(full_train))\n",
    "val_size   = len(full_train) - train_size\n",
    "\n",
    "train_ds, val_ds = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "print(f\"Train size:      {len(train_ds)}\")\n",
    "print(f\"Validation size: {len(val_ds)}\")\n",
    "print(f\"Test size:       {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81494df3",
   "metadata": {},
   "source": [
    "# load the train, validation and test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23237671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of output classes for OxfordIIITPet\n",
    "NUM_CLASSES = 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d34de",
   "metadata": {},
   "source": [
    "# create dataloader handler from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")\n",
    "print(f\"Test batches:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faaba75",
   "metadata": {},
   "source": [
    "# now create the cnn, use the resnet18 as backbone, and ResNet18_Weights as initial weights.\n",
    "- create the backbone\n",
    "- create a classifier \n",
    "- based on your dataset add the correct number of output classes\n",
    "- the classifier have to be trainable.\n",
    "\n",
    "# Question 1?\n",
    "- Why we freeze the backbone? Why not the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_classes: int,\n",
    "                 freeze_backbone: bool = True,\n",
    "                 dense_units: list[int] = [],\n",
    "                 dropout_prob: float = 0.3,\n",
    "                 weights=ResNet18_Weights.DEFAULT):\n",
    "        super().__init__()\n",
    "        self.weights = weights\n",
    "\n",
    "        # Load pretrained ResNet18 as backbone\n",
    "        backbone = resnet18(weights=weights)\n",
    "        \n",
    "        # Remove the final FC layer, keep everything else\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.backbone_out = 512  # ResNet18 outputs 512 features\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Build MLP classifier with multiple dense layers\n",
    "        mlp = []\n",
    "        cur = self.backbone_out\n",
    "        for h in dense_units:\n",
    "            mlp += [\n",
    "                nn.Linear(cur, h),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropout_prob)\n",
    "            ]\n",
    "            cur = h  # Update for next layer\n",
    "        \n",
    "        self.classifier = nn.Sequential(*mlp)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.final_classifier = nn.Linear(cur, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)           # (batch, 512, 1, 1)\n",
    "        x = x.flatten(start_dim=1)     # (batch, 512)\n",
    "        x = self.classifier(x)         # (batch, dense_units[-1] or 512)\n",
    "        x = self.final_classifier(x)   # (batch, num_classes)\n",
    "        return x\n",
    "    \n",
    "    def unfreeze_layer4(self):\n",
    "        \"\"\"Unfreeze the last ResNet block for fine-tuning\"\"\"\n",
    "        for param in self.backbone[7].parameters():  # layer4\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def get_transform(self):\n",
    "        return self.weights.transforms()\n",
    "\n",
    "\n",
    "# Create model with empty dense_units (direct 512 -> 37 mapping)\n",
    "model = TransferModel(num_classes=NUM_CLASSES, freeze_backbone=True, dense_units=[]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q1_answer",
   "metadata": {},
   "source": [
    "## Answer to Question 1\n",
    "\n",
    "We **freeze the backbone** because it was already trained on ImageNet and has learned rich, general-purpose features (edges, textures, shapes). Freezing it means those weights are not updated during training, which:\n",
    "1. Saves a lot of computation — only the small classifier is updated.\n",
    "2. Prevents destroying the learned features with a noisy gradient from our small dataset.\n",
    "\n",
    "We do **not freeze the classifier** because it is randomly initialised and knows nothing about our dataset. It needs to be trained from scratch to map the backbone's features to our 37 pet-breed classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b0eea2",
   "metadata": {},
   "source": [
    "# add the train evaluation, and predict functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    \"\"\"Train for one epoch, return average loss and accuracy.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    \"\"\"Evaluate on a dataloader, return average loss and accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    \"\"\"Train for multiple epochs, return history of losses and accuracies.\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs,   val_accs   = [], []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss,   val_acc   = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "\n",
    "def predict(model, loader):\n",
    "    \"\"\"Run inference on a dataloader, return all predictions and true labels.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    return np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3fe8c5",
   "metadata": {},
   "source": [
    "### Phase 1: Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba96147",
   "metadata": {},
   "source": [
    "# freeze all layers except the classifier.\n",
    "- train and evaluate the model for 50 epochs \n",
    "- remember to save val loss and train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770fba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backbone is already frozen in __init__ (freeze_backbone=True)\n",
    "# Only trainable parameters are in classifier and final_classifier\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train for 50 epochs using the train() function\n",
    "phase1_train_losses, phase1_val_losses, phase1_train_accs, phase1_val_accs = train(\n",
    "    model, train_loader, val_loader, criterion, optimizer, epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a232ace",
   "metadata": {},
   "source": [
    "# Plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0cc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, 51)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(epochs_range, phase1_train_losses, label=\"Train Loss\")\n",
    "axes[0].plot(epochs_range, phase1_val_losses,   label=\"Val Loss\")\n",
    "axes[0].set_title(\"Phase 1 – Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(epochs_range, phase1_train_accs, label=\"Train Acc\")\n",
    "axes[1].plot(epochs_range, phase1_val_accs,   label=\"Val Acc\")\n",
    "axes[1].set_title(\"Phase 1 – Accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1fac57",
   "metadata": {},
   "source": [
    "# plot predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0be5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a grid of 8 test images with predicted vs true class names\n",
    "class_names = test_ds.classes  # list of 37 breed names\n",
    "\n",
    "# Grab one batch from the test loader\n",
    "images_batch, labels_batch = next(iter(test_loader))\n",
    "outputs = model(images_batch.to(device))\n",
    "preds_batch = outputs.argmax(dim=1).cpu()\n",
    "\n",
    "# ImageNet normalisation used by ResNet – we reverse it for display\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std  = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = images_batch[i].permute(1, 2, 0) * std + mean\n",
    "    img = img.clamp(0, 1).numpy()\n",
    "    ax.imshow(img)\n",
    "    true_name = class_names[labels_batch[i]]\n",
    "    pred_name = class_names[preds_batch[i]]\n",
    "    color = \"green\" if labels_batch[i] == preds_batch[i] else \"red\"\n",
    "    ax.set_title(f\"True: {true_name}\\nPred: {pred_name}\", color=color, fontsize=8)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Phase 1 Predictions (green = correct, red = wrong)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870efa9",
   "metadata": {},
   "source": [
    "# calculate TEST accuracy score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1_preds, phase1_labels = predict(model, test_loader)\n",
    "phase1_test_acc = accuracy_score(phase1_labels, phase1_preds)\n",
    "print(f\"Phase 1 Test Accuracy: {phase1_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792c955",
   "metadata": {},
   "source": [
    "# Calculate confusion matrices precision and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(phase1_labels, phase1_preds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "im = ax.imshow(cm, cmap=\"Blues\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_title(\"Phase 1 – Confusion Matrix\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class precision and recall (macro average for a single summary number)\n",
    "precision_macro = precision_score(phase1_labels, phase1_preds, average=\"macro\", zero_division=0)\n",
    "recall_macro    = recall_score(phase1_labels, phase1_preds,    average=\"macro\", zero_division=0)\n",
    "\n",
    "print(f\"Phase 1 Macro Precision: {precision_macro:.4f}\")\n",
    "print(f\"Phase 1 Macro Recall:    {recall_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9246e",
   "metadata": {},
   "source": [
    "### Phase 2: Freeze layer 4 - Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e747ec4",
   "metadata": {},
   "source": [
    "# from the freezed cnn unfreeze the  ``` layer4 ```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unfreeze_layer4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the built-in method to unfreeze layer4\n",
    "model.unfreeze_layer4()\n",
    "\n",
    "# Create optimizer with differential learning rates\n",
    "# Lower LR for backbone, higher LR for classifier\n",
    "optimizer = optim.Adam([\n",
    "    {\"params\": model.backbone[7].parameters(),    \"lr\": 1e-4},  # layer4\n",
    "    {\"params\": model.classifier.parameters(),     \"lr\": 1e-3},\n",
    "    {\"params\": model.final_classifier.parameters(), \"lr\": 1e-3},\n",
    "])\n",
    "\n",
    "print(\"layer4 and classifier are now trainable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e471d8",
   "metadata": {},
   "source": [
    "# Train for 50 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac75948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 50 epochs using the train() function\n",
    "phase2_train_losses, phase2_val_losses, phase2_train_accs, phase2_val_accs = train(\n",
    "    model, train_loader, val_loader, criterion, optimizer, epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bf91b",
   "metadata": {},
   "source": [
    "# Plot curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(epochs_range, phase2_train_losses, label=\"Train Loss\")\n",
    "axes[0].plot(epochs_range, phase2_val_losses,   label=\"Val Loss\")\n",
    "axes[0].set_title(\"Phase 2 – Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(epochs_range, phase2_train_accs, label=\"Train Acc\")\n",
    "axes[1].plot(epochs_range, phase2_val_accs,   label=\"Val Acc\")\n",
    "axes[1].set_title(\"Phase 2 – Accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fb601",
   "metadata": {},
   "source": [
    "# visualize prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b55bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the same batch for a fair visual comparison with Phase 1\n",
    "outputs = model(images_batch.to(device))\n",
    "preds_batch = outputs.argmax(dim=1).cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = images_batch[i].permute(1, 2, 0) * std + mean\n",
    "    img = img.clamp(0, 1).numpy()\n",
    "    ax.imshow(img)\n",
    "    true_name = class_names[labels_batch[i]]\n",
    "    pred_name = class_names[preds_batch[i]]\n",
    "    color = \"green\" if labels_batch[i] == preds_batch[i] else \"red\"\n",
    "    ax.set_title(f\"True: {true_name}\\nPred: {pred_name}\", color=color, fontsize=8)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Phase 2 Predictions (green = correct, red = wrong)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7278848",
   "metadata": {},
   "source": [
    "# Calculate test accuracy score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2_preds, phase2_labels = predict(model, test_loader)\n",
    "phase2_test_acc = accuracy_score(phase2_labels, phase2_preds)\n",
    "print(f\"Phase 1 Test Accuracy: {phase1_test_acc:.4f}\")\n",
    "print(f\"Phase 2 Test Accuracy: {phase2_test_acc:.4f}\")\n",
    "print(f\"Improvement:           {phase2_test_acc - phase1_test_acc:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f5498",
   "metadata": {},
   "source": [
    "# calculate confusion matrix precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2_cm",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(phase2_labels, phase2_preds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "im = ax.imshow(cm2, cmap=\"Blues\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_title(\"Phase 2 – Confusion Matrix\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "precision_macro2 = precision_score(phase2_labels, phase2_preds, average=\"macro\", zero_division=0)\n",
    "recall_macro2    = recall_score(phase2_labels, phase2_preds,    average=\"macro\", zero_division=0)\n",
    "\n",
    "print(f\"Phase 2 Macro Precision: {precision_macro2:.4f}\")\n",
    "print(f\"Phase 2 Macro Recall:    {recall_macro2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c0e9c",
   "metadata": {},
   "source": [
    "## Answer to Question 2 – What is the difference between transfer learning and fine tuning?\n",
    "\n",
    "**Transfer learning** (Phase 1) means taking a model that was pre-trained on a large dataset (ImageNet) and re-using its feature-extraction layers as-is, frozen. Only a new classification head on top is trained. This is fast and works well even with limited data, because the frozen layers already know how to detect general visual features.\n",
    "\n",
    "**Fine tuning** (Phase 2) goes one step further: after the classifier has been trained, we unfreeze some of the later backbone layers (here `layer4`) and continue training with a *lower* learning rate. This allows the model to adapt the high-level features of the backbone to the specifics of our dataset, usually pushing accuracy higher. We use a small learning rate so we adjust the pretrained weights gently rather than overwriting them.\n",
    "\n",
    "**Key takeaway:** Transfer learning gets you a good baseline quickly; fine tuning squeezes out extra performance by letting the network specialise its features to your task. The trade-off is that fine tuning takes more time and requires careful learning rate selection to avoid destabilising the pretrained weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_course_2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
