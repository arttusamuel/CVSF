{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9483d3ce",
   "metadata": {},
   "source": [
    "### Assignment 2 Understanding transfer learning and fine tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03187ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "print(f\"[INFO] Torch version: {torch.__version__}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_class",
   "metadata": {},
   "source": [
    "# Model Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18Transfer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 number_of_output_classes: int,\n",
    "                 freeze_backbone: bool = True,\n",
    "                 dense_units: list[int] = [256],\n",
    "                 dropout_probabilities: float = 0.3,\n",
    "                 weights=ResNet18_Weights.DEFAULT):\n",
    "        super().__init__()\n",
    "        self.weights = weights\n",
    "\n",
    "        # Create a feature extractor\n",
    "        backbone = resnet18(weights=weights)\n",
    "        self.features = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.backbone_out = backbone.fc.in_features  # 512 for ResNet18\n",
    "\n",
    "        # Freeze the layers\n",
    "        if freeze_backbone:\n",
    "            for p in self.features.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Classifier\n",
    "        mlp = []\n",
    "        cur = self.backbone_out\n",
    "        for h in dense_units:\n",
    "            mlp += [\n",
    "                nn.Linear(cur, h),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropout_probabilities),\n",
    "            ]\n",
    "            cur = h  # Update current size\n",
    "\n",
    "        # Add final classifier\n",
    "        self.classifier = nn.Sequential(*mlp)\n",
    "        self.final_classifier = nn.Linear(cur, number_of_output_classes)\n",
    "\n",
    "        # Ensure head trainable\n",
    "        for p in self.classifier.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in self.final_classifier.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return self.final_classifier(x)\n",
    "    \n",
    "    def unfreeze_layer4(self):\n",
    "        \"\"\"Unfreeze the last ResNet block for fine-tuning\"\"\"\n",
    "        # features[7] is layer4 in ResNet18\n",
    "        for p in self.features[7].parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def get_transform(self):\n",
    "        \"\"\"This returns the recommended preprocessing for the pretrained weights\"\"\"\n",
    "        return self.weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper_functions",
   "metadata": {},
   "source": [
    "# Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device, scaler):\n",
    "    \"\"\"Train for one epoch with mixed precision.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate the model on a dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, \n",
    "                              device, scaler, epochs, patience=5):\n",
    "    \"\"\"Train with early stopping.\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, scaler)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n[EARLY STOP] No improvement for {patience} epochs. Stopping at epoch {epoch}.\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    \"\"\"Get predictions for entire dataset.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    return np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e681a3e",
   "metadata": {},
   "source": [
    "# Dataset Configuration and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Configuration\n",
    "cnn_configuration = {\n",
    "    \"number_of_output_classes\": 37,  # OxfordIIITPet has 37 classes\n",
    "    \"freeze_backbone\": True,\n",
    "    \"dense_units\": [],  # Empty = direct 512 -> 37 mapping\n",
    "    \"dropout_probabilities\": 0.3\n",
    "}\n",
    "\n",
    "# Create model and get transform\n",
    "model = ResNet18Transfer(**cnn_configuration).to(device)\n",
    "transform = model.get_transform()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9644c7a",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de646ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "full_train = datasets.OxfordIIITPet(root=\"./data\", split=\"trainval\", download=True, transform=transform)\n",
    "test_ds = datasets.OxfordIIITPet(root=\"./data\", split=\"test\", download=True, transform=transform)\n",
    "\n",
    "# Train/val split\n",
    "val_ratio = 0.2\n",
    "val_size = int(len(full_train) * val_ratio)\n",
    "train_size = len(full_train) - val_size\n",
    "train_ds, val_ds = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "print(f\"Train size:      {len(train_ds)}\")\n",
    "print(f\"Validation size: {len(val_ds)}\")\n",
    "print(f\"Test size:       {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d34de",
   "metadata": {},
   "source": [
    "# create dataloader handler from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                         num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                       num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")\n",
    "print(f\"Test batches:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faaba75",
   "metadata": {},
   "source": [
    "# Question 1?\n",
    "- Why we freeze the backbone? Why not the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q1_answer",
   "metadata": {},
   "source": [
    "## Answer to Question 1\n",
    "\n",
    "We **freeze the backbone** because it was already trained on ImageNet and has learned rich, general-purpose features (edges, textures, shapes). Freezing it means those weights are not updated during training, which:\n",
    "1. Saves a lot of computation — only the small classifier is updated.\n",
    "2. Prevents destroying the learned features with a noisy gradient from our small dataset.\n",
    "\n",
    "We do **not freeze the classifier** because it is randomly initialised and knows nothing about our dataset. It needs to be trained from scratch to map the backbone's features to our 37 pet-breed classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3fe8c5",
   "metadata": {},
   "source": [
    "### Phase 1: Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba96147",
   "metadata": {},
   "source": [
    "# freeze all layers except the classifier.\n",
    "- train and evaluate the model for 50 epochs \n",
    "- remember to save val loss and train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770fba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer and loss\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                      lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Train with early stopping\n",
    "print(\"\\n=== Phase 1: Transfer Learning ===\")\n",
    "phase1_train_losses, phase1_val_losses, phase1_train_accs, phase1_val_accs = train_with_early_stopping(\n",
    "    model, train_loader, val_loader, criterion, optimizer, device, scaler, epochs=30, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a232ace",
   "metadata": {},
   "source": [
    "# Plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0cc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, len(phase1_train_losses) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(epochs_range, phase1_train_losses, label=\"Train Loss\")\n",
    "axes[0].plot(epochs_range, phase1_val_losses, label=\"Val Loss\")\n",
    "axes[0].set_title(\"Phase 1 – Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(epochs_range, phase1_train_accs, label=\"Train Acc\")\n",
    "axes[1].plot(epochs_range, phase1_val_accs, label=\"Val Acc\")\n",
    "axes[1].set_title(\"Phase 1 – Accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1fac57",
   "metadata": {},
   "source": [
    "# plot predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0be5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a grid of 8 test images with predicted vs true class names\n",
    "class_names = test_ds.classes\n",
    "\n",
    "# Grab one batch from the test loader\n",
    "images_batch, labels_batch = next(iter(test_loader))\n",
    "outputs = model(images_batch.to(device))\n",
    "preds_batch = outputs.argmax(dim=1).cpu()\n",
    "\n",
    "# ImageNet normalisation - reverse it for display\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = images_batch[i].permute(1, 2, 0) * std + mean\n",
    "    img = img.clamp(0, 1).numpy()\n",
    "    ax.imshow(img)\n",
    "    true_name = class_names[labels_batch[i]]\n",
    "    pred_name = class_names[preds_batch[i]]\n",
    "    color = \"green\" if labels_batch[i] == preds_batch[i] else \"red\"\n",
    "    ax.set_title(f\"True: {true_name}\\nPred: {pred_name}\", color=color, fontsize=8)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Phase 1 Predictions (green = correct, red = wrong)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870efa9",
   "metadata": {},
   "source": [
    "# calculate TEST accuracy score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase1_preds, phase1_labels = predict(model, test_loader, device)\n",
    "phase1_test_acc = accuracy_score(phase1_labels, phase1_preds)\n",
    "print(f\"Phase 1 Test Accuracy: {phase1_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792c955",
   "metadata": {},
   "source": [
    "# Calculate confusion matrices precision and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(phase1_labels, phase1_preds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "im = ax.imshow(cm, cmap=\"Blues\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_title(\"Phase 1 – Confusion Matrix\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class precision and recall\n",
    "precision_macro = precision_score(phase1_labels, phase1_preds, average=\"macro\", zero_division=0)\n",
    "recall_macro = recall_score(phase1_labels, phase1_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(f\"Phase 1 Macro Precision: {precision_macro:.4f}\")\n",
    "print(f\"Phase 1 Macro Recall:    {recall_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9246e",
   "metadata": {},
   "source": [
    "### Phase 2: Freeze layer 4 - Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e747ec4",
   "metadata": {},
   "source": [
    "# from the freezed cnn unfreeze the  ``` layer4 ```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unfreeze_layer4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze layer4\n",
    "model.unfreeze_layer4()\n",
    "\n",
    "# CRITICAL: Use MUCH lower learning rates for fine-tuning\n",
    "optimizer = optim.Adam([\n",
    "    {\"params\": model.features[7].parameters(), \"lr\": 1e-5},      # layer4 - very low\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": 5e-4},       # MLP head\n",
    "    {\"params\": model.final_classifier.parameters(), \"lr\": 5e-4}, # final layer\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "scaler = GradScaler()  # Reset scaler\n",
    "\n",
    "print(\"layer4 and classifier are now trainable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e471d8",
   "metadata": {},
   "source": [
    "# Train for 50 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac75948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with early stopping\n",
    "print(\"\\n=== Phase 2: Fine-Tuning ===\")\n",
    "phase2_train_losses, phase2_val_losses, phase2_train_accs, phase2_val_accs = train_with_early_stopping(\n",
    "    model, train_loader, val_loader, criterion, optimizer, device, scaler, epochs=30, patience=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bf91b",
   "metadata": {},
   "source": [
    "# Plot curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range2 = range(1, len(phase2_train_losses) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(epochs_range2, phase2_train_losses, label=\"Train Loss\")\n",
    "axes[0].plot(epochs_range2, phase2_val_losses, label=\"Val Loss\")\n",
    "axes[0].set_title(\"Phase 2 – Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(epochs_range2, phase2_train_accs, label=\"Train Acc\")\n",
    "axes[1].plot(epochs_range2, phase2_val_accs, label=\"Val Acc\")\n",
    "axes[1].set_title(\"Phase 2 – Accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fb601",
   "metadata": {},
   "source": [
    "# visualize prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b55bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the same batch for comparison\n",
    "outputs = model(images_batch.to(device))\n",
    "preds_batch = outputs.argmax(dim=1).cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = images_batch[i].permute(1, 2, 0) * std + mean\n",
    "    img = img.clamp(0, 1).numpy()\n",
    "    ax.imshow(img)\n",
    "    true_name = class_names[labels_batch[i]]\n",
    "    pred_name = class_names[preds_batch[i]]\n",
    "    color = \"green\" if labels_batch[i] == preds_batch[i] else \"red\"\n",
    "    ax.set_title(f\"True: {true_name}\\nPred: {pred_name}\", color=color, fontsize=8)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Phase 2 Predictions (green = correct, red = wrong)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7278848",
   "metadata": {},
   "source": [
    "# Calculate test accuracy score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2_preds, phase2_labels = predict(model, test_loader, device)\n",
    "phase2_test_acc = accuracy_score(phase2_labels, phase2_preds)\n",
    "print(f\"Phase 1 Test Accuracy: {phase1_test_acc:.4f}\")\n",
    "print(f\"Phase 2 Test Accuracy: {phase2_test_acc:.4f}\")\n",
    "print(f\"Improvement:           {phase2_test_acc - phase1_test_acc:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f5498",
   "metadata": {},
   "source": [
    "# calculate confusion matrix precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2_cm",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(phase2_labels, phase2_preds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "im = ax.imshow(cm2, cmap=\"Blues\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_title(\"Phase 2 – Confusion Matrix\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "precision_macro2 = precision_score(phase2_labels, phase2_preds, average=\"macro\", zero_division=0)\n",
    "recall_macro2 = recall_score(phase2_labels, phase2_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(f\"Phase 2 Macro Precision: {precision_macro2:.4f}\")\n",
    "print(f\"Phase 2 Macro Recall:    {recall_macro2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c0e9c",
   "metadata": {},
   "source": [
    "# Question 2 What did you learn? What is the difference between transfer learning and fine tuning? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q2_answer",
   "metadata": {},
   "source": [
    "## Answer to Question 2\n",
    "\n",
    "**Transfer learning** (Phase 1) means taking a model that was pre-trained on a large dataset (ImageNet) and re-using its feature-extraction layers as-is, frozen. Only a new classification head on top is trained. This is fast and works well even with limited data, because the frozen layers already know how to detect general visual features.\n",
    "\n",
    "**Fine tuning** (Phase 2) goes one step further: after the classifier has been trained, we unfreeze some of the later backbone layers (here `layer4`) and continue training with a *much lower* learning rate. This allows the model to adapt the high-level features of the backbone to the specifics of our dataset, usually pushing accuracy higher. We use a very small learning rate (1e-5 for layer4 vs 1e-3 in Phase 1) to avoid catastrophic forgetting — if the learning rate is too high, the model will overfit to the training set and destroy the valuable pretrained features.\n",
    "\n",
    "**Key takeaway:** Transfer learning gets you a good baseline quickly; fine tuning squeezes out extra performance by letting the network specialise its features to your task. The critical requirement for fine-tuning is using conservative learning rates (typically 10-100x lower than Phase 1) to prevent overfitting and preserve pretrained knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_course_2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
